{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Algorithms**\n",
    "In this project, I used Proximal Policy Optimization (PPO, Shulman2017) to solve Reacher Environment. PPO aims to solve a major limitation in policy gradients methods: data inefficeincy. Each trajectory can validly be used for updating the policy network once in PG, which is wasteful, especially when the generation process is slow, resource-consuming or even dangerous. With tricks of importance sampling, surrogate objectives, and surrogate clipping, the policy network can then be updated multiple times using a generated trajectory (generated from an \"old policy\") without losing track from the true objective function. This technique enhances data effiency greatly by creating off-policy learning (improving a policy other than the trajectory generating one) alike capability for PG algorithm. \n",
    "\n",
    "### **Implementation**\n",
    "My implementation is based on the idea of Algorithm.xx in John Schulman's 2017 paper, the algorithm x.x. Learning rules are summarized here:\n",
    "\n",
    "In addition, PPO algorithm has parallel learning nature in that one can use a policy to generate simultaneously multiple trajectories and learn from all of them afterwards. Therefore, I chose multi-agent version of the environment to take advantage of it. I created a trajectory buffer using torch.utils.data to organize the data format and mini-batch generation. \n",
    "\n",
    "### **Results**  \n",
    "\n",
    "#### **Statistics**\n",
    "**Discussion**\n",
    "\n",
    "**Video recording of a trained agent**\n",
    "\n",
    "### **Future Work**\n",
    "- compare the result with other PG based methods\n",
    "- play with the latest Soft-Actor Critic.\n",
    "\n",
    "### **Reference**\n",
    "Research Papers:\n",
    "- [Proximal Policy Optimization 2017](https://www.nature.com/articles/nature14236)\n",
    "- [Dueling DQN 2016](https://arxiv.org/abs/1511.06581)\n",
    "- [Double DQN 2016](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "Related Projects:\n",
    "- [tnakae: Udacity-DeepRL-p2-Continuous]\n",
    "(https://github.com/tnakae/Udacity-DeepRL-p2-Continuous)\n",
    "\n",
    "\n",
    "\n",
    "### **Appendix**\n",
    "1. Key equations and the corresponding lines of codes in the project are summarized in ![./equations.png](./equations.png) \n",
    "2. Hyperparameters\n",
    "\n",
    "| Hyperparameter                      | Value |\n",
    "| ----------------------------------- | ----- |\n",
    "| Agent Model Type                    | MLP   |\n",
    "| Agent Model Arch                    | [in, 20, 20, out] |\n",
    "| Update (Learning) Frequency         | every 4 steps |\n",
    "| Replay buffer size                  | 1e5   |\n",
    "| Batch size                          | 64    |\n",
    "| $\\gamma$ (discount factor)          | 0.99  |\n",
    "| Optimizer                           | Adam  |\n",
    "| Learning rate                       | 5e-4  |\n",
    "| Soft-Update (*1)                      | True  |\n",
    "| $\\tau$ (soft-update mixing rate)    | 1e-3  |\n",
    "| $\\epsilon$ (exploration rate) start | 1.0   |\n",
    "| $\\epsilon$ minimum                  | 0.1   |\n",
    "| $\\epsilon$ decay                    | 0.995 |\n",
    "\n",
    "(*1.) Soft-update is a way to update the target network by mixing the current local parameters and current target parameters one via the following formula:\n",
    "\n",
    "$w_{target} \\leftarrow \\tau * w_{local} + (1-\\tau) * w_{target}$\n",
    "\n",
    "The original DQN uses hard-update (i.e. $\\tau=1$), meaning that the target network is fully updated by the current local network. In practice, hard-update needs to be updated less frequently since it requires the local network to first accumulate more knowledge. The choice of hard-update is exposed in the project, and one can experiment with it by setting soft-update to be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "[image1]: ./ppo_future_rewards-exp1.png \"Crawler\"\n",
    "\n",
    "- Using PPO to solve Reacher environment:\n",
    "- Exp-1: v1: Future Reward PPO/ Reinforce PPO => not working: \n",
    "    - Reinforce PPO ![][image1]\n",
    "    - hyperparams = {\"max_t\": 1000,\n",
    "                   \"SGD_epoch\": 2,\n",
    "                   \"gamma\": 0.99,\n",
    "                   \"n_episodes\": 150,\n",
    "                   \"grad_clip\": 0.2,\n",
    "                   \"epsilon\": 0.01,\n",
    "                   \"beta\": 0.01}  # \n",
    "- Exp-2: using single agent environment:\n",
    "    - Exp-2.1: Find bug, the entropy is to maximize but not minimize...\n",
    "    - it is still not working, it is able to learn something, but it looks like agent can't try our something meaningful!\n",
    "        - reward_to_go is not working!\n",
    "        - reinforce is ...? not working! \n",
    "        -     hyperparams = {\"max_t\": 1000, # maximal possible t_step is only 1000\n",
    "                   \"SGD_epoch\": 2,\n",
    "                   \"gamma\": 0.99,\n",
    "                   \"n_episodes\": 150,\n",
    "                   \"grad_clip\": 0.0,\n",
    "                   \"epsilon\": 0.01,\n",
    "                   \"beta\": 0.01}\n",
    "              setting = {\"multi_env\": False,\n",
    "                   \"plotting\": False}\n",
    "       - with gradient clip\n",
    "\n",
    "- Exp-3: using tnakae's implementation:\n",
    "    - exp3.1 it is working~~ yay (PPO with GAE)\n",
    "    - exp3.2 it is not working if I am using return as the value (last R using value function), it reaches ~11 in episode 150 and stuck here.\n",
    "    - exp3.3 using R_last=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
